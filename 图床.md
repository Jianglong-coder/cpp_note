### 分布式图床介绍

项目利用多个中间件实现的一个分布式图床项目，其中使用nginx做反向代理和负载均衡，mysql数据库来存储关系型数据，redis来存储用户的token，使用fastDFS来充当图床的分布式存储引擎，使用jsoncpp来对post内容的json串进行解析，用log4cplus来实现日志的记录，并用llhttp来对http请求的解析，实现了一个分布式的图床系统，主要有一下几个功能，分别是注册、登录、文件妙传、文件共享、文件下载排行榜。（proactor模型，主线程读取数据，业务交给子线程处理）

#### 压测

单线程有索引随机插入10000条数据qbs约1304 随机查询1000次qbs约6993 无索引随机插入10000条数据qbs约1318 随机查询1000次qbs约150

#### 技术选型

这里存储引擎选用的fastDFS，首先是因为我这里做的是图床，一般都是一些小文件，fastDFS在小文件存储上有很大的优势，有专门的小文件存储策略，而ceph比较适用于大文件存储，在大文件上比较有优势，而且fastDFS天生支持分布式，使得扩展性得到了保证，其次就是选用redis来存储一些热点内容，这里主要考虑到redis有丰富的数据结构，使得对数据的操作变得非常容易，比如排行榜功能，还有就是redis带有持久化功能，项目中的token数据是可以持久化到磁盘中的，即使服务器宕机了，还是可以恢复的，还有就是redis的分布式功能了，如若分享的文件变多，使用redis可以很简单的扩展。选用nginx是nginx是高性能的http服务器，支持反向代理和负载均衡，在后期如果并发量增加，可以使用nginx实现应用层负载均衡，(使用llhttp来做http的解析，主要考虑到了llhttp的高性能和社区比较活跃，http_parse已经停止维护了。)

#### 功能实现

- 文件秒传：原理就是上传的时候先向服务器发送文件的md5，msyql有一张file_info的表，这张表中存储着已上传文件的一些信息，其中包含了文件的md5值，如果中存在此文件的md5值，那么只需copy一下mysql表中的数据即可实现文件秒传，如果不存在那么就继续上传即可。
- 图片分享：mysql中有一个share_picture_list表，其中存储的是图片的md5，和一个url标识符，这个标识符是随机生成的，我们如果想要访问分享的图片就可以在url中加上这个图片的标识符，然后后端就会根据标识符来联合查询file_info表得到图片的真实url。
- token验证：利用redis中的hset来实现用户token的存储，第一次成功登录后，会生成一个token返回给客户端，然后在redis中对token进行存储，用户名为key，token就是value，之后的每次访问都使用token来做验证
- 下载排行榜：使用redis的zset数据结构，实现了根据共享文件的下载量实现排行榜
- 文件上传，使用了nginx-upload模块，用户提交post表单，nginx把文件存储到临时目录上，然后把文件元数据发送给后端程序，后端程序把文件上传到fastDFS服务器上面，然后删除临时文件
- 文件下载：这里使用了fastDFS的扩展模块，每个storage节点上部署nginx，由nginx来提供下载文件的功能，根据上传的文件id来获取文件，如果文件不在当前节点，则向源storage发起重定向或者代理

#### 项目优化

1. 我在一些频繁使用where查询的mysql字段做了索引来优化查询功能，因为文件的查询，插入都需要操作表file_info中的文件md5，因此对md5进行建立前缀索引，来优化查询；还会根据用户名来查询密码，因此在user_file中对用户名建立索引，文件上传的时候会根据user_file_list中的用户名 md5和文件名唯一确定一条记录，因此对这三个列建立联合索引，等等。  
2. 并对一些热点数据用redis做了一层缓存，来提升性能，因此每个连接都会分配一个token，登录后token全程携带，因此用redis来缓存，文件数量缓存到了redis，因为如若用户文件很多，计算文件数量非常耗时，因此使用redis来进行缓存。

#### fastDFS架构

- 客户端：
  - 上传文件：先向tarcker进行通信，tracker返回storage的信息，然后客户端把文件上传到storage节点
  - 下载文件：先向tracker进行通信，tracker返回要下载文件所在的storage节点的信息，客户端进行下载
- Tracker 跟踪器集群
  - 主要是是维护storage集群的信息，这里storage会定期发送心跳包给tracker，其中包括storage的信息
  - 这里每个tracker节点都是对等的，客户端访问任何一个tracker都可行，多个tracker节点避免了单点故障问题，实现系统的高可用性
- Storage集群
  - Storage集群按照组进行划分，组与组之间是横向扩容关系，来提升集群的容量
  - 组内是纵向扩容，用来做数据冗余备份，避免单点故障，保证高可用性

#### 小文件存储（Trunk文件机制）

首先使能小文件存储功能，后续的文件小于16M就使用trunk文件存储，使用trunk文件（默认64M）存储生成的文件id就发生了变化，id中加入了trunk文件的文件id和小文件的在trunk文件的偏移，小文件在trunk中使用trunkHeader（描述了文件信息）+文件内容来存储，Storage内部会使用我实现的内存池一样来管理空闲存储空间，不过不同大小链表之间是使用平衡树组织的，当无剩余时，重新构建一个trunk文件。

#### fastDFS 组内同步策略

这里用到了binlog，每个storage节点都有一个binlog，里面记录了文件的信息，同步过程中就是用到这个binlog，storage是不存有group内其他storage的信息的，是通过向tracker发送心跳包收到的回复中得到的，对组内每一个storage开启一个同步线程进行同步，并且对组内每一个storage有一个mark文件，标记同步进度。这里有一个最小同步时间，会定期跟tracker汇报，tracker根据这个时间判断storage中是否有存在某个文件

#### 负载均衡策略

##### nginx

- 轮询策略（默认负载均衡策略）
- 最少连接数负载均衡策略（惊群现象自己处理，上锁，谁取到锁谁就accept，因为要实现负载均衡的粗略，系统处理惊群就不能负载均衡了）
- ip-hash 负载均衡策略
- 权重负载均衡策略，这个是依概率实现的

##### fastDFS

- tracker
  - 这里所有的tracker是对等的，可以选择任意一个tracker
- group
  - Round robin，所有的group间轮询
  - Specified group，指定某一个确定的group
  - Load balance，选择最大剩余空 间的组上传文件
- storage
  - Round robin，在group内的所有storage间轮询。
  - First server ordered by ip，按IP排序，也会轮询。
  - First server ordered by priority，按优先级排序（优先级在storage上配置）；可以理解为权重的方式，权重高的优先选择。比如，storage 1 为100M的带宽和storage 2为500M带宽，上传文件时优先使用500M带宽的storage。
- storage path
  - Round robin，多个存储目录间轮询。
  - 剩余存储空间最多的优先。



#### 和对象存储的区别（ceph）

- 首先这个fastDFS的分布式存储引擎，属于一个文件系统，使用目录树来组织文件的，用户需要根据具体的目录和文件名来获取文件，就和普通的文件系统无异
- 而对象存储是近几年才出现的技术，对象存储没有目录级别，直接根据文件唯一id就可以获取文件，把文件当作一个对象存储在服务器中。
- 采用文件系统的存储引擎存储数据是一个层级的结构，而对象存储是扁平化的结构



### 可能的面试题

- 分布式锁实现：简单实现set nx expire   改进redis的**redlock**
  - zookeeper实现分布式锁，采用临时有序节点来实现，当客户端需要加锁的时候向zookeeper申请一个临时有序节点，申请完之后，查看自己的节点是不是第一个节点，如果是第一个节点，那么就上锁成功，如果不是，说明已经有其他客户端上锁成功了，此时zookeeper采用watch命令进行监听比自己小的节点删除事件，如果删除，那么通知客户端上锁成功。

- raft协议：
- CAP理论![image-20240131161215079](C:\Users\root\AppData\Roaming\Typora\typora-user-images\image-20240131161215079.png)
- Ceph  [Ceph的存储原理_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Wm4y1V74n/?spm_id_from=333.337.search-card.all.click&vd_source=3aa535caf6a287f00d045ee5ffb0228d)  会把文件分片成**对象存储**（osd），均匀分布到每个节点的每个硬盘，也就是一个文件的存储也可以负载均衡到不同节点，fastDFS不可以。因此Ceph在大文件存储上很有优势，fastDFS在小文件存储有很大优势（尤其是trunk策略）
- 一致性hash：将hash值组织成一个抽象的环，hash环，节点在环上，数据映射到环上，顺时针存到节点中，引入虚拟节点避免数据分布不均
- java垃圾回收，可达性算法，根据可达性算法来判断对象是否还会被使用





### raft

#### 什么是raft算法

Raft算法是一种共识算法，用于在分布式系统中实现一致性。它是由Diego Ongaro和John Ousterhout于2013年提出的，旨在提供一种更易理解和可靠的分布式一致性算法。

Raft算法解决了分布式系统中的领导者选举、日志复制和安全性等关键问题。它将分布式系统中的节点划分为 **领导者（leader）、跟随者（follower）和候选者（candidate）** 三种角色，并通过一个选举过程来选择领导者。

在Raft算法中，领导者负责接收客户端的请求，并将请求复制到其他节点的日志中。跟随者和候选者则通过与领导者保持心跳和选举的方式来保持一致性。如果领导者失去联系或无法正常工作，系统会触发新一轮的选举过程，选择新的领导者。

Raft算法的设计目标是可理解性和可靠性。相比于其他共识算法如Paxos，Raft算法更加直观和易于理解，使得开发人员能够更容易地实现和调试分布式系统。

#### raft优缺点

Raft算法作为一种共识算法，在分布式系统中具有一些优点和缺点。

**优点：**

1. **简单易懂**：相比于其他共识算法，Raft算法的设计更加直观和易于理解，使得开发人员能够更容易地实现和调试分布式系统。
2. **安全性**：Raft算法保证了系统的安全性，通过领导者选举和日志复制等机制来确保数据的一致性和可靠性。
3. **高可用性**：Raft算法能够在领导者失效时快速进行新的领导者选举，从而保证系统的高可用性。

**缺点：**

1. **性能开销**：Raft算法对于每个写操作都需要进行日志复制，这会带来一定的性能开销。相比于其他共识算法如Paxos，Raft算法的性能可能会稍差一些。
2. **领导者单点故障**：在Raft算法中，领导者是负责处理客户端请求和日志复制的节点，如果领导者失效，整个系统的性能和可用性都会受到影响。
3. **数据一致性延迟**：在Raft算法中，当领导者发生变更时，新的领导者需要等待日志复制完成才能处理客户端请求，这可能会导致一定的数据一致性延迟。



#### 算法原理

Raft算法的核心原理包括三个关键组件：领导者选举、日志复制和安全性。

**1. 领导者选举：**

- 每个节点在任意时刻可能处于三种状态之一：领导者（leader）、跟随者（follower）和候选者（candidate）。
- 初始情况下，所有节点都是跟随者。如果一个跟随者在一段时间内没有收到来自领导者的心跳消息，它会转变为候选者并开始选举过程。（无所谓  就是选举 自己的原因也选举）
- 候选者会向其他节点发送投票请求，并在收到多数节点的选票后成为新的领导者。
- 如果在选举过程中出现多个候选者获得相同票数的情况，那么会进行新一轮的选举，直到只有一个候选者获胜。

**2. 日志复制：**

- Raft算法使用日志来记录系统中的所有操作。每个节点都有一个日志，其中包含一系列的日志条目。
- 当客户端向领导者发送写请求时，领导者会将该请求作为一个新的日志条目追加到自己的日志中，并向其他节点发送日志复制请求。
- 其他节点收到复制请求后，会将该日志条目追加到自己的日志中，并向领导者发送确认消息。
- 一旦领导者收到多数节点的确认消息，该日志条目被视为已提交，并将其应用到状态机中执行相应操作。

**3. 安全性：**

- Raft算法通过多数投票机制来确保系统的安全性。任何一条已提交的日志条目都必须在多数节点上复制和执行，才能保证数据的一致性。
- 如果一个节点成为领导者，并开始复制日志条目，但在复制完成之前失去了领导者地位，那么新的领导者将继续复制剩余的日志条目。
- 如果一个节点在复制过程中发现自己的日志与领导者的日志不一致，它将回退到领导者的日志状态，并重新进行复制。

总的来说，Raft算法通过领导者选举、日志复制和安全性机制，实现了分布式系统中的一致性和可靠性。它的设计简单易懂，易于实现，并且提供了强一致性保证。



#### 领导者选举

1. 初始状态下，所有节点都是跟随者（Follower）状态。
2. 如果一个跟随者在一段时间内没有收到来自领导者（Leader）的心跳消息，它会转变为候选者（Candidate）并开始选举过程。
3. 候选者向其他节点发送投票请求，并请求其他节点投票给自己。
4. 其他节点在收到投票请求后，如果还没有投票给其他候选者，且候选者的日志更新且比自己的日志新，就会投票给候选者。
5. 如果候选者收到了多数节点的选票（包括自己的一票），那么它就成为新的领导者。
6. 如果在选举过程中出现多个候选者获得相同票数的情况，那么会进行新一轮的选举，直到只有一个候选者获胜。

通过以上步骤，Raft 算法实现了分布式系统中的领导者选举机制，确保系统能够选出稳定的领导者来协调其他节点的操作。